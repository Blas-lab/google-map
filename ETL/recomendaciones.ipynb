{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO DE MACHINE LEARNING MEDIANTE LIBRERIA SURPRISE PARA RECOMENDACION\n",
    "DE RESTAURANTES BASADA EN LOS RATINGS OTORGADOS POR LOS USUARIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías Pandas y Numpy para trabajar con dataframes y Glob y Os para\n",
    "# el manejo de archivos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, unificaremos los datasets de reviews obtenidos de GCP en un solo archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un listado de los archivos a unir recorriendo todos los archivos del path\n",
    "# indicado y filtrando por el nombre pasado como parámetro con el uso de un comodín\n",
    "joined_files = os.path.join(\"./datasets\", \"exportaciones_reviews*.csv\")\n",
    "joined_list = glob.glob(joined_files)\n",
    "  \n",
    "# Importamos todos los archivos y los concatenamos en un único dataframe\n",
    "reviews = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos ahora el dataset con los datos de los comercios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el dataset\n",
    "comercios = pd.read_csv(\"./datasets/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas que no utilizaremos en el modelo\n",
    "reviews = reviews.drop([\"time\",\"text\",\"resp\",\"state\",\"resp_time\",\"resp_text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 329540 entries, 0 to 329539\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   name              329539 non-null  object\n",
      " 1   address           328805 non-null  object\n",
      " 2   gmap_id           329540 non-null  object\n",
      " 3   description       22924 non-null   object\n",
      " 4   latitude          329540 non-null  object\n",
      " 5   longitude         329540 non-null  object\n",
      " 6   avg_rating        329540 non-null  object\n",
      " 7   num_of_reviews    329540 non-null  object\n",
      " 8   price             49841 non-null   object\n",
      " 9   hours             251993 non-null  object\n",
      " 10  relative_results  269126 non-null  object\n",
      " 11  url               329540 non-null  object\n",
      " 12  cat_id            329540 non-null  object\n",
      " 13  city_id           252539 non-null  object\n",
      "dtypes: object(14)\n",
      "memory usage: 35.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Revisamos la composición del dataframe obtenido\n",
    "comercios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas adicionales\n",
    "comercios = comercios.drop([\"latitude\",\"longitude\",\"price\",\"hours\",\"relative_results\",\"url\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a agregar ahora al dataframe de los comercios, datos adicionales como la categoría (por ejemplo, pizzeria) y el grupo (por ejemplo, alojamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la tabla de categorías\n",
    "categorias = pd.read_csv(\"./datasets/categorias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la tabla de grupos\n",
    "grupos = pd.read_csv(\"./datasets/groups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un inner join de categorias con grupos para luego unirla a comercios\n",
    "categorias = categorias.merge(grupos, on=\"group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos al dataframe de los comercios las categorias y grupos de cada uno de ellos\n",
    "comercios = comercios.merge(categorias, on=\"cat_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, prepararemos el dataframe a introducir en el modelo del sistema\n",
    "de recomendación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos únicamente los sitios del grupo \"comida\", descartando \"ocio\" y \"alojamiento\"\n",
    "comercios_comida = comercios[comercios[\"description_y\"] == \"Comida\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos al dataframe de reviews los datos de cada comercio \n",
    "reviews_completo = reviews.merge(comercios_comida, on=\"gmap_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la tabla de ciudades para incorporlas luego a la tabla de comercios\n",
    "# y poder filtrar en el modelo por la localidad seleccionada por el usuario\n",
    "cities = pd.read_csv(\"./datasets/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos el datrafame el modelo de ML. Al utilizar la librería Surprise, solo se\n",
    "# pasan los datos relativos al usuario que hizo la review, el id del comercio visitado\n",
    "# y el puntaje otorgado\n",
    "reviews_modelo = reviews_completo[[\"user_id\",\"gmap_id\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomando en cuenta que usaremos el filtro de ciudad para seleccionar una recomendación,\n",
    "# eliminamos los registros que no tienen ese dato\n",
    "comercios_comida.dropna(subset=['city_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name              object\n",
       "address           object\n",
       "gmap_id           object\n",
       "description       object\n",
       "avg_rating        object\n",
       "num_of_reviews    object\n",
       "cat_id            object\n",
       "city_id            int64\n",
       "description_x     object\n",
       "group_id           int64\n",
       "description_y     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiamos el tipo de dato para la columna \"city_id\" a efectos de poder \n",
    "# efectuar el join posterior\n",
    "comercios_comida.astype({'city_id': 'int64'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el nombre de la ciudad al dataframe de comercios\n",
    "comercios_comida = comercios_comida.merge(cities, on=\"city_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos ahora a trabajar directamente en el modelo de Machine Learning (ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las dos librerías necesarias para nuestro modelo de ML de\n",
    "# sistema de recomendación. \n",
    "# Surprise para efectuar las predicciones y Reader para indicarle como\n",
    "# debe leer el dataframe que le ingestaremos. En este caso, le indicamos\n",
    "# que el valor de los ratings va de 1 a 5\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Ingestamos en el modelo de ML el dataset, con los 3 datos requeridos por Surprise\n",
    "# para efectuar las predicciones\n",
    "data = Dataset.load_from_df(reviews_modelo[['user_id', 'gmap_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    1.0423  1.0417  1.0413  1.0418  0.0004  \n",
      "MAE (testset)     0.7864  0.7860  0.7859  0.7861  0.0002  \n",
      "Fit time          14.63   17.85   16.87   16.45   1.34    \n",
      "Test time         5.91    6.09    6.61    6.20    0.30    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.04233623, 1.04166673, 1.04131331]),\n",
       " 'test_mae': array([0.78638152, 0.78595958, 0.78593282]),\n",
       " 'fit_time': (14.633838415145874, 17.84606695175171, 16.872409343719482),\n",
       " 'test_time': (5.9072065353393555, 6.090474843978882, 6.614306926727295)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos la librería SVD y la de Validación cruzada para instanciar nuestro modelo\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Instanciamos nuestro modelo y efectuamos una validación cruzada, obteniendo\n",
    "# los diferentes resultados para cada instancia\n",
    "svd = SVD(verbose=True, n_epochs=10)\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x280f278da80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=111293913029428063847, iid='0x89c6e40d71f55e27:0x9cc01b88f4ed07f8', r_ui=None, est=4.606893948087677, details={'was_impossible': False})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Efectuamos una predicción de muestra, para un usuario y un negocio en particular\n",
    "# En este caso, la predicción del rating para este usuario y negocio es de 4.6\n",
    "svd.predict(uid=111293913029428063847, iid=\"0x89c6e40d71f55e27:0x9cc01b88f4ed07f8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, exportaremos el modelo entrenado y el dataset utilizado para que\n",
    "se carguen en otra instancia y sean puestos en producción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería Pickle que nos permite guardar el modelo entrenado en un \n",
    "# archivo de extensión pkl\n",
    "import pickle\n",
    "pickle.dump(svd,open(\"model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos el dataset final de los comercios del grupo \"comida\" para ser\n",
    "# utilizado en otra instancia\n",
    "comercios_comida.to_csv(\"comercios_comida.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a8ac931c2da658386087c1aeacfbaae947468c457a926ada196b02a9b560fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
